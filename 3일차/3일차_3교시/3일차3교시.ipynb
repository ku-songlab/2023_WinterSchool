{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ---\n",
        "- Project: 2023 Winter School\n",
        "- Author: Gyu-min Lee\n",
        "- Version: 0.5\n",
        "- Changelog\n",
        "    - 0.1 -- Initiated the file\n",
        "    - 0.5 -- First Draft\n",
        "--- -->"
      ],
      "metadata": {
        "id": "Q9gzdxqAsnsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2023 전산언어학 겨울학교 3일차 3교시\n",
        "\n",
        "# Language Models "
      ],
      "metadata": {
        "id": "sKVGZDums7tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project: NSMC Classification with 🤗 Transformers\n",
        "\n",
        "- NSMC는 Naver의 영화 리뷰를 기반으로 구축된 웹 텍스트 기반 감성 분석 텍스트입니다\n",
        "- 여기서는 🤗 Model Hub에서 일반적인 한국어 텍스트로 구축된 BERT 모델과, 댓글로 구축한 BERT 모델을 각각 불러와 Fine-Tuning하여 결과를 비교해 봅니다\n",
        "- 📔NOTE: 빠른 실행을 위해 Runtime 유형을 'GPU'로 해 주세요"
      ],
      "metadata": {
        "id": "tBXi4nFXH3se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "metadata": {
        "id": "O9LP93mMIU5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.26.0 "
      ],
      "metadata": {
        "id": "kp5yCyN_H3c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP0: Load libraries\n",
        "import csv \n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "PAqjQ3okIipN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP1: Prepare models\n",
        "BERT_GENERAL_NAME = \"snunlp/KR-BERT-char16424\"\n",
        "# https://huggingface.co/snunlp/KR-BERT-char16424\n",
        "BERT_COMMENT_NAME = \"beomi/kcbert-base\"\n",
        "# https://huggingface.co/beomi/kcbert-base\n",
        "\n",
        "bert_general = AutoModelForSequenceClassification.from_pretrained(BERT_GENERAL_NAME,\n",
        "                                                                  num_labels=2)\n",
        "bert_general_tknizer = AutoTokenizer.from_pretrained(BERT_GENERAL_NAME)\n",
        "\n",
        "bert_comment = AutoModelForSequenceClassification.from_pretrained(BERT_COMMENT_NAME,\n",
        "                                                                  num_labels=2)\n",
        "bert_comment_tknizer = AutoTokenizer.from_pretrained(BERT_COMMENT_NAME)"
      ],
      "metadata": {
        "id": "hZZFdn97KF_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_general"
      ],
      "metadata": {
        "id": "A7QCgNdfLdhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "BertForSequenceClassification??"
      ],
      "metadata": {
        "id": "5zPurbd0Lgoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP2: Prepare the data \n",
        "\n",
        "DATA_PATH = './nsmc/ratings.txt'\n",
        "\n",
        "with open(DATA_PATH) as f:\n",
        "    nsmc_reader = csv.reader(f, delimiter='\\t')\n",
        "    \n",
        "    nsmc = list()\n",
        "\n",
        "    for row in list(nsmc_reader)[1:]:\n",
        "        nsmc.append({\"inputs\": row[1],\n",
        "                     \"labels\": int(row[2])})\n",
        "\n",
        "class NsmcDataset(Dataset):\n",
        "    def __init__(self, processed_data, tokenizer):\n",
        "        self.processed_data = processed_data\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.processed_data[idx]['inputs']\n",
        "        input_ids = self.tokenizer(inputs, truncation=True).input_ids\n",
        "        labels = self.processed_data[idx]['labels']\n",
        "\n",
        "        return {\n",
        "            \"inputs\": inputs, \n",
        "            \"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "def construct_datasets(dataset: list, tokenizer, random_state: int=263) -> dict:\n",
        "    \"\"\"split dataset into train-dev-test sets in the ratio of 0.7, 0.2, 0.1\n",
        "\n",
        "    Will also return the data into NsmcDatset instances with input tokenized with tokenizer\n",
        "    \"\"\"\n",
        "\n",
        "    train, others = train_test_split(dataset, test_size=0.3, random_state=random_state)\n",
        "    dev, test = train_test_split(others, test_size=1/3, random_state=random_state)\n",
        "\n",
        "    return {\"train\": NsmcDataset(train, tokenizer),\n",
        "            \"dev\": NsmcDataset(dev[:int(len(dev)/100)], tokenizer),\n",
        "            # the size of 'dev' is reduced only for demonstration purpose\n",
        "            \"test\": NsmcDataset(test, tokenizer)}\n",
        "    \n",
        "nsmc_general = construct_datasets(nsmc, bert_general_tknizer)\n",
        "nsmc_comment = construct_datasets(nsmc, bert_comment_tknizer)\n"
      ],
      "metadata": {
        "id": "b5ct727vLPQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_comment_tknizer.tokenize('이런거 정말 읽을수 있는 거임? ㅋㅋㅋ')"
      ],
      "metadata": {
        "id": "Rjy4AxjJNqrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP3: Set up a trainer\n",
        "\n",
        "training_args_general = TrainingArguments(num_train_epochs=0.1,\n",
        "                                        # probably increase epochs for better result\n",
        "                                          output_dir='./checkpoints/general',\n",
        "                                          per_device_train_batch_size=64,\n",
        "                                          per_device_eval_batch_size=128,\n",
        "                                          evaluation_strategy='steps', # or, 'epoch'\n",
        "                                          logging_dir='./checkpoints/general/logs',\n",
        "                                          logging_steps=50,\n",
        "                                          save_steps=100,\n",
        "                                          load_best_model_at_end=True,\n",
        "                                        #   no_cuda=True\n",
        "                                        )\n",
        "\n",
        "training_args_comment = TrainingArguments(num_train_epochs=0.1,\n",
        "                                        # probably increase epochs for better result\n",
        "                                          output_dir='./checkpoints/comment',\n",
        "                                          per_device_train_batch_size=64,\n",
        "                                          per_device_eval_batch_size=128,\n",
        "                                          evaluation_strategy='steps', # or, 'epoch'\n",
        "                                          logging_dir='./checkpoints/comment/logs',\n",
        "                                          logging_steps=50,\n",
        "                                          save_steps=100,\n",
        "                                          load_best_model_at_end=True,\n",
        "                                          # no_cuda=True\n",
        "                                        )"
      ],
      "metadata": {
        "id": "tIX1nsisPojX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(model_output) -> dict:\n",
        "    labels = model_output.label_ids\n",
        "    predictions = model_output.predictions.argmax(-1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "\n",
        "    return {\"accuracy\": accuracy,\n",
        "            \"f1\": f1}"
      ],
      "metadata": {
        "id": "Cp0XBIY9TUcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_general = Trainer(model=bert_general,\n",
        "                          args=training_args_general,\n",
        "                          train_dataset=nsmc_general[\"train\"], \n",
        "                          eval_dataset=nsmc_general[\"dev\"],\n",
        "                          tokenizer=bert_general_tknizer,\n",
        "                          compute_metrics=metrics,\n",
        "                          )\n",
        "\n",
        "trainer_comment = Trainer(model=bert_comment,\n",
        "                          args=training_args_comment,\n",
        "                          train_dataset=nsmc_comment[\"train\"], \n",
        "                          eval_dataset=nsmc_comment[\"dev\"],\n",
        "                          tokenizer=bert_comment_tknizer,\n",
        "                          compute_metrics=metrics,\n",
        "                          )"
      ],
      "metadata": {
        "id": "8TD5BmkqToiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP4: train\n",
        "\n",
        "# check GPU availability\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "yt9wbwKOUOTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_general.train()"
      ],
      "metadata": {
        "id": "jHHDmlvTUYcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_comment.train()"
      ],
      "metadata": {
        "id": "XCQ-CGKAUfk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP5: predict\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "pipe_general = pipeline(task='text-classification',\n",
        "                model=bert_general,\n",
        "                tokenizer=bert_general_tknizer,\n",
        "                device=DEVICE,\n",
        "                )\n",
        "\n",
        "pipe_comment = pipeline(task='text-classification',\n",
        "                model=bert_comment,\n",
        "                tokenizer=bert_comment_tknizer,\n",
        "                device=DEVICE,\n",
        "                )\n",
        "\n",
        "test_inputs_general = [nsmc_general['test'][idx]['inputs'] for idx in range(len(nsmc_general['test']))]\n",
        "test_labels_general = [nsmc_general['test'][idx]['labels'] for idx in range(len(nsmc_general['test']))]\n",
        "test_inputs_comment = [nsmc_comment['test'][idx]['inputs'] for idx in range(len(nsmc_comment['test']))]\n",
        "test_labels_comment = [nsmc_comment['test'][idx]['labels'] for idx in range(len(nsmc_comment['test']))]\n",
        "\n",
        "test_inputs_general = random.sample(test_inputs_general, 100)\n",
        "test_labels_general = random.sample(test_labels_general, 100)\n",
        "test_inputs_comment = random.sample(test_inputs_comment, 100)\n",
        "test_labels_comment = random.sample(test_labels_comment, 100)\n",
        "\n",
        "test_preds_general = pipe_general(test_inputs_general)\n",
        "test_preds_comment = pipe_comment(test_inputs_comment)\n",
        "\n"
      ],
      "metadata": {
        "id": "_cuZtcJShJB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_general[:10]"
      ],
      "metadata": {
        "id": "cUYNEg9xkNcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_comment[:10]"
      ],
      "metadata": {
        "id": "sdm99uaV327m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_for_pipe_result(pipe_res, labels):\n",
        "    predictions = [0 if res[\"label\"] == \"LABEL_0\" else 1 for res in pipe_res]\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "\n",
        "    return {\"accuracy\": accuracy,\n",
        "            \"f1\": f1}"
      ],
      "metadata": {
        "id": "OEb51fub337t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_for_pipe_result(test_preds_general, test_labels_general)"
      ],
      "metadata": {
        "id": "ZxGhMyH15d9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_for_pipe_result(test_preds_comment, test_labels_comment)"
      ],
      "metadata": {
        "id": "hg8bE_hA5iXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PRINT = 10 \n",
        "\n",
        "sample_indices = random.sample(range(len(test_preds_general)), NUM_PRINT)\n",
        "\n",
        "for sample_index in sample_indices:\n",
        "    print(f\"Result for index: {sample_index}\")\n",
        "    print(\"Prediction on:\")\n",
        "    print(f\"\\t{test_inputs_general[sample_index]}\")\n",
        "    print(\"Answer:\")\n",
        "    print(f\"\\t{'POS' if test_labels_general[sample_index]==1 else 'NEG'}\")\n",
        "    print(\"Prediction with general bert:\")\n",
        "    print(f\"\\t{'POS' if test_preds_general[sample_index]['label']=='LABEL_1' else 'NEG'}\", end='  ')\n",
        "    print(f\"\\t{test_preds_general[sample_index]['score']:04.2f}\")\n",
        "    print(\"Prediction with comment bert:\")\n",
        "    print(f\"\\t{'POS' if test_preds_comment[sample_index]['label']=='LABEL_1' else 'NEG'}\", end='  ')\n",
        "    print(f\"\\t{test_preds_comment[sample_index]['score']:04.2f}\")\n",
        "    print('\\n')"
      ],
      "metadata": {
        "id": "k4kpXduM5mLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRA: save and load trained model\n",
        "\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"./checkpoints/comment/checkpoint-100\", # 모델이 저장된 경로명\n",
        "    )\n",
        "\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"./checkpoints/comment/checkpoint-100\", # 모델이 저장된 경로명\n",
        "    )\n"
      ],
      "metadata": {
        "id": "GALIQE2T8Jnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_loaded = pipeline(task='text-classification',\n",
        "                model=loaded_model,\n",
        "                tokenizer=loaded_tokenizer,\n",
        "            )\n",
        "\n",
        "pipe_loaded(\"이런 댓글도 해석할 수 있으려나요...?\")\n"
      ],
      "metadata": {
        "id": "HiV76JH8NYwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r './model_save.zip' './checkpoints/comment/checkpoint-100'"
      ],
      "metadata": {
        "id": "nu23ZuxZOGcO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}