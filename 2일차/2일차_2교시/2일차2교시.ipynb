{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySYSZZt09tXI"
      },
      "source": [
        "# 2일차 2교시 워드임베딩 - 실습\n",
        "한선아"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contents\n",
        "1. Word2vec\n",
        "2. FastText\n",
        "3. Glove\n",
        "4. Visualization"
      ],
      "metadata": {
        "id": "OnH8uKNBsnhF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0 실습 준비하기"
      ],
      "metadata": {
        "id": "lAdwVn7GuIh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 코랩 파일 업로드"
      ],
      "metadata": {
        "id": "UINZxvop2fqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `wiki_small800.txt` : 한국어 위키 텍스트\n",
        "* `NanumBarunpenB.otf` : 시각화 시 한글이 깨지지 않도록 사용하는 폰트\n",
        "* `kor_ws353` : 단어 유사도(word similarity) 테스트 데이터\n",
        "* `kor_analogy_semantic` : 의미론적 단어 유추 테스트 데이터"
      ],
      "metadata": {
        "id": "Mw8_6--pIv9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라이브러리 import"
      ],
      "metadata": {
        "id": "bK8rFpNw2l4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFUQZ6JJ9tXS"
      },
      "outputs": [],
      "source": [
        "# 사용할 라이브러리들을 import 합니다.\n",
        "import gensim \n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.models import KeyedVectors\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Gensim 라이브러리-[What is Gensim?](https://radimrehurek.com/gensim/intro.html)\n",
        "  * 텍스트 데이터를, 의미를 가진 벡터로 표현하기 위해 필요한 기능들을 지원하는 라이브러리입니다.    \n",
        "  Gensim을 통해 Word2Vec, FastText와 같은 알고리즘을 사용할 수 있습니다."
      ],
      "metadata": {
        "id": "WBpweEsENYIr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unhsPHNg9tXX"
      },
      "source": [
        "### 코퍼스 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2qiOZGB9tXY"
      },
      "outputs": [],
      "source": [
        "# 사용할 코퍼스의 경로\n",
        "path = '/content/wiki_small800.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_GmiPy99tXZ"
      },
      "outputs": [],
      "source": [
        "# 가져온 데이터를 확인해보겠습니다.\n",
        "df = pd.read_csv(path, encoding=\"utf-8\", header=None)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9QbF36W9tXd"
      },
      "outputs": [],
      "source": [
        "# 첫 번째 문서\n",
        "df[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qeH3IVw9tXe"
      },
      "outputs": [],
      "source": [
        "# 다섯 번째 문서\n",
        "df[0][4]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 워드 임베딩 모델"
      ],
      "metadata": {
        "id": "31vNfea0uUrz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hbi6tqHp9tXf"
      },
      "source": [
        "### 1) Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DAkx8QC9tXg"
      },
      "source": [
        "* 2013년 구글 연구팀이 발표한 기법으로, 가장 널리 쓰이고 있는 단어 임베딩 모델입니다.\n",
        "* Word2Vec 모델의 학습방법 2가지\n",
        "    * **CBOW 모델** : 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습됩니다.\n",
        "    * **Skip-gram 모델** : 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측하는 과정에서 학습됩니다.\n",
        "<img src=\"https://blog.kakaocdn.net/dn/Czgg5/btqEttXkz91/LK5RqukCujicrxQ2kRWt0k/img.png\" height=300>\n",
        "* [Gensim API Reference - Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ASp2L89tXi"
      },
      "source": [
        "#### Word2Vec 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYFf8FuA9tXk"
      },
      "outputs": [],
      "source": [
        "# 모델에 입력값으로 사용할 수 있도록 적합한 객체로 바꿉니다.\n",
        "corpus = gensim.models.word2vec.Text8Corpus(path)\n",
        "\n",
        "print(list(corpus)[0][:100])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가져온 데이터(corpus)로 Word2Vec 모델을 학습시킵니다.\n",
        "model = Word2Vec(sentences=corpus, size=100, window=3, min_count=5, sg=1)"
      ],
      "metadata": {
        "id": "uPho16QcgaNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA_pWVyU9tXo"
      },
      "source": [
        "* `sentences` : 단어리스트 (a list of lists of tokens)\n",
        "* `size` : 임베딩 벡터의 차원\n",
        "* `window` : 중심단어를 예측하기 위해서 앞, 뒤로 볼 단어의 개수(범위)\n",
        "* `min_count` : 빈도수가 얼마나 작으면 제외(ignore)할 지 결정\n",
        "* `sg` : 훈련 알고리즘. 1 - skip-gram, 0 - CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97d1G9-Q9tXq"
      },
      "source": [
        "#### Word2Vec 임베딩 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpH69i_n9tXr"
      },
      "outputs": [],
      "source": [
        "# 모델의 학습 결과로 얻은 워드 벡터들을 저장해둡니다.\n",
        "word2vec_vectors = model.wv\n",
        "word2vec_vectors.save(\"word2vec.wordvectors\")\n",
        "\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GnbYFJa9tXs"
      },
      "outputs": [],
      "source": [
        "# '컴퓨터'에 해당하는 밀집 벡터를 확인해보겠습니다. \n",
        "word2vec_vectors['컴퓨터'] # 코퍼스에 존재하는 단어"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DxQO7PG9tXt"
      },
      "source": [
        "#### Word2Vec 임베딩 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epFC-z579tXu"
      },
      "outputs": [],
      "source": [
        "# 저장해두었던 워드벡터들을 불러옵니다.\n",
        "word2vec_vectors = KeyedVectors.load(\"word2vec.wordvectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoPnCimA9tXv"
      },
      "source": [
        "##### 가장 유사한 단어 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6Blqczr9tXw"
      },
      "outputs": [],
      "source": [
        "# '컴퓨터'와 가장 유사한 단어 10개 출력\n",
        "word2vec_vectors.most_similar('컴퓨터', topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스에 존재하지 않는 단어는 오류가 납니다.\n",
        "word2vec_vectors.most_similar('전산언어학겨울학교', topn=10)"
      ],
      "metadata": {
        "id": "43FwHm_YmgZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HFLDMzN9tXx"
      },
      "source": [
        "##### 두 단어의 유사도 계산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9LYl-ox9tXy"
      },
      "outputs": [],
      "source": [
        "# '컴퓨터'와 '유럽'의 유사도\n",
        "word2vec_vectors.similarity('컴퓨터', '유럽')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WWG2kmc9tXz"
      },
      "outputs": [],
      "source": [
        "# '컴퓨터'와 '웹'의 유사도\n",
        "word2vec_vectors.similarity('컴퓨터', '웹')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l82x-3Rr9tX0"
      },
      "source": [
        "##### 가장 유사하지 않은 단어 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8k791je9tX1"
      },
      "outputs": [],
      "source": [
        "# '일본', '중국', '미국' 중 가장 유사하지 않은 단어 출력\n",
        "word2vec_vectors.doesnt_match(['일본','중국', '미국'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EapurJ0E9tX2"
      },
      "source": [
        "##### 단어벡터의 연산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPwqT2fn9tX3"
      },
      "outputs": [],
      "source": [
        "# 왕 + 여성 - 남성 = ???\n",
        "word2vec_vectors.most_similar(positive=['왕', '여성'], negative=['남성'], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UowaZcwY9tX4"
      },
      "source": [
        "### 2) FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1myOp9A9tX5"
      },
      "source": [
        "* 페이스북에서 개발해 공개한 단어 임베딩 기법\n",
        "* word2vec과 기본적으로 동일하나, 각 단어를 문자(Character) 단위 n-gram으로 표현합니다.\n",
        "* FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주합니다. 내부 단어, 즉 서브워드(subword)를 고려하여 학습합니다.\n",
        "* 코퍼스에 없는 모르는 단어(Out Of Vocabulary)에도 대처할 수 있다는 장점이 있습니다.\n",
        "* [Gensim API Reference - FastText](https://radimrehurek.com/gensim/models/fasttext.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FastText 모델 학습"
      ],
      "metadata": {
        "id": "AhG1KdhkJGAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 가져온 데이터(corpus)로 FastText 모델을 학습시킵니다.\n",
        "model = FastText(sentences=corpus, size=100, window=3, min_count=5, sg=1)"
      ],
      "metadata": {
        "id": "yIK1cGDw-vZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9FLiBH1jstW"
      },
      "source": [
        "* `sentences` : 단어리스트 (a list of lists of tokens)\n",
        "* `size` : 임베딩 벡터의 차원\n",
        "* `window` : 중심단어를 예측하기 위해서 앞, 뒤로 볼 문자의 개수(범위)\n",
        "* `min_count` : 빈도수가 얼마나 작으면 제외(ignore)할 지 결정\n",
        "* `sg` : 훈련 알고리즘. 1 - skip-gram, 0 - CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FastText 임베딩 결과 확인"
      ],
      "metadata": {
        "id": "BGR7JVYnJMgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 학습 결과로 얻은 워드벡터들을 저장해둡니다.\n",
        "FastText_vectors = model.wv\n",
        "FastText_vectors.save(\"fasttext.wordvectors\")\n",
        "\n",
        "del model"
      ],
      "metadata": {
        "id": "__0SeqAOIOgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  '컴퓨터'에 해당하는 밀집 벡터를 확인해보겠습니다.\n",
        "FastText_vectors['컴퓨터']"
      ],
      "metadata": {
        "id": "zoOnYk9ADevH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FastText 임베딩 평가하기"
      ],
      "metadata": {
        "id": "E_9k-l2nKCWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장해두었던 워드벡터들을 불러옵니다.\n",
        "FastText_vectors = KeyedVectors.load(\"fasttext.wordvectors\")"
      ],
      "metadata": {
        "id": "OZHDJogQJxJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 가장 유사한 단어 출력"
      ],
      "metadata": {
        "id": "0HkQL0lsVxg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastText_vectors.most_similar('컴퓨터', topn=10)"
      ],
      "metadata": {
        "id": "4-lTc2Z-Vc2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스에 없는 단어도 유사도를 계산할 수 있습니다.\n",
        "FastText_vectors.most_similar('전산언어학겨울학교', topn=10)"
      ],
      "metadata": {
        "id": "A439Omp4m6Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 두 단어의 유사도 분석"
      ],
      "metadata": {
        "id": "54FRBHVaVtUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastText_vectors.similarity('미국', '영국')"
      ],
      "metadata": {
        "id": "17z5j6YwVNtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastText_vectors.similarity('미국', '함수')"
      ],
      "metadata": {
        "id": "UvxpmzH34V_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Glove\n",
        "* 미국 스탠포드대학교연구팀에서 개발한 단어 임베딩 기법\n",
        "* 유사도 계산의 성능이 좋으면서도, 윈도우 내의 로컬문맥(local context)만 학습하지 않고 전체의 통계정보를 반영하고자 고안된 기법입니다.\n",
        "* 단어-문맥 행렬(동시 등장 행렬, co-occurrence matrix)을 사용합니다.\n",
        " * 오늘 뭐 먹고 싶어\n",
        " * 나는 오늘 연어 먹고 싶어\n",
        " * 나는 어제 연어 먹었어\n",
        "\n",
        "\n",
        "  | 카운트 | 오늘 | 뭐  | 먹고 | 싶어 | 나는 | 연어 | 어제 | 먹었어 |\n",
        "  | ------ | ---- | --- | ---- | ---- | ---- | ---- | ---- | ------ |\n",
        "  | 오늘   |  0    |  1  |   0   |  0    |  1   | 1    |  0    |    0    |\n",
        "  | 뭐     |  1   |  0   |   1  |    0  |  0    |   0   |    0 |   0     |\n",
        "  | 먹고   |   0   |  1  |  0    |   2  |  0    |   1  |   0   |    0    |\n",
        "  | 싶어   |  0    |   0  |    2 |    0  |  0    |   0   |    0  |   0     |\n",
        "  | 나는   |    1 |   0  |  0    |  0    |    0  |   0   |  1   |     0   |\n",
        "  | 연어   | 1    |  0   |  1   |   0   | 0     |   0   |   1  |    1   |\n",
        "  | 어제   |  0    |   0  |   0   |     0 |    1 |   1  |     0 |   0     |\n",
        "  | 먹었어 |   0   |  0   |  0    |   0   |    0  | 1    |  0    |  0      |\n",
        "\n",
        "* [glove-python GIthub](https://github.com/maciejkula/glove-python)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vv2LLPSoWPqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Glove를 사용하기 위해 라이브러리를 설치합니다.\n",
        "! pip install glove-python-binary"
      ],
      "metadata": {
        "id": "F9p8pD5WsqTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리를 import 합니다.\n",
        "from glove import Glove, Corpus"
      ],
      "metadata": {
        "id": "Wo9utNYlnvhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 단어-문맥 행렬 만들기"
      ],
      "metadata": {
        "id": "1QUjULnCYULD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스 객체를 선언합니다. \n",
        "data = gensim.models.word2vec.Text8Corpus(path)\n",
        "corpus = Corpus()"
      ],
      "metadata": {
        "id": "hwW0-jkEhLhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스 어휘 사전과 동시등장행렬(단어-문맥행렬)을 생성합니다.\n",
        "corpus.fit(data, window=3)\n",
        "\n",
        "# 코퍼스 안의 딕셔너리 크기와, 연어의 개수를 출력합니다.\n",
        "print('Dict size : %s' % len(corpus.dictionary))\n",
        "print('Collocations: %s' % corpus.matrix.nnz)       # nnz : non-zero element\n",
        "\n",
        "# 코퍼스를 corpus.model로 저장합니다.\n",
        "corpus.save('corpus.model')"
      ],
      "metadata": {
        "id": "ixNOg5jRh0q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Glove 모델 학습하기"
      ],
      "metadata": {
        "id": "LbMCcoPaY1Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Glove 모델을 선언합니다.\n",
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "\n",
        "# Glove 모델을 학습합니다.\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)"
      ],
      "metadata": {
        "id": "Uc-BeDJ8dYuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `no_components` : 임베딩벡터의 차원\n",
        "* `learning_rate` : 학습률 - 모델을 업데이트할 때 사용하는 보폭\n",
        "* `epoch` : 에포크- 학습 횟수"
      ],
      "metadata": {
        "id": "VFkhuetUz0Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Glove 모델을 사용하기 위해서는 모델에 해당 단어 사전 올려주어야 합니다.\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "\n",
        "# 모델을 저장합니다.\n",
        "glove.save('glove.model')"
      ],
      "metadata": {
        "id": "zsvv_AKUxme4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Glove 모델 평가"
      ],
      "metadata": {
        "id": "fz4B9Eu5eie_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove = Glove.load('glove.model')\n",
        "glove.most_similar('언어', number=10)"
      ],
      "metadata": {
        "id": "FdAkuDFmezQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 Visualization\n",
        "\n",
        "* word2vec 임베딩 결과를 시각화합니다.\n",
        "* 임베딩 벡터의 차원을 100차원으로 했기 때문에, 시각화를 위해 우리가 이해할 수 있는 2차원, 3차원의 저차원으로 축소해야합니다.\n",
        "*  t-SNE(t-distributed Stochastic Neighbor Embedding)\n",
        "  * 차원 축소 시에, 단어간의 거리가 가깝고 먼 정도를 최대한 보존하기 위한 방법론\n",
        "  * 원 공간의 데이터 확률 분포와 축소된 공간의 분포 사이의 차이를 최소화하는 방향으로 벡터 공간을 조금씩 바꿔나갑니다.\n",
        "\n",
        "* [t-SNE 개념과 사용법 참고](https://gaussian37.github.io/ml-concept-t_sne/)"
      ],
      "metadata": {
        "id": "_8VRR3ISiv9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.cm as cm"
      ],
      "metadata": {
        "id": "kxBfWizJn36A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화에서 한글이 깨지지 않도록 폰트를 올려줍니다.\n",
        "path_nanum = \"/content/NanumBarunpenB.otf\"\n",
        "prop = fm.FontProperties(fname=path_nanum)\n",
        "\n",
        "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
        "mpl.rcParams['axes.unicode_minus'] = False "
      ],
      "metadata": {
        "id": "HhJuxPGkmgk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단한 전처리를 합니다.\n",
        "FILTERS = \"([~,!?\\\"':.;|~)^(])\"\n",
        "CHANGE_FILTER = re.compile(FILTERS)\n",
        "\n",
        "EXP = \"[1234567890\\-]\"\n",
        "CHANGE_EXP = re.compile(EXP)\n",
        "\n",
        "ENG = \"[a-zA-Z]\"\n",
        "CHANGE_ENG = re.compile(ENG)\n",
        "\n",
        "#df = pd.read_csv(path, header=None)\n",
        "words = []\n",
        "for line in df[0]:\n",
        "    line = CHANGE_FILTER.sub(\"\", line)\n",
        "    line = CHANGE_EXP.sub(\"\", line)\n",
        "    line = CHANGE_ENG.sub(\"\", line)\n",
        "    token = word_tokenize(line)\n",
        "    words+=token"
      ],
      "metadata": {
        "id": "BCpOF3TZmyxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word2vec 임베딩 결과를 불러옵니다.\n",
        "word2vec_wordvectors = KeyedVectors.load('word2vec.wordvectors')\n",
        "\n",
        "# 가장 빈도수가 높은 800개의 단어를 추출합니다.\n",
        "freq_list = Counter(words).most_common(800)\n",
        "vocab = [i[0] for i in freq_list if len(i[0])>1]\n",
        "\n",
        "# 해당 단어들에 해당하는 임베딩 벡터\n",
        "X = word2vec_wordvectors[vocab]"
      ],
      "metadata": {
        "id": "mSy6tk22m_0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) tsne 2차원 축소"
      ],
      "metadata": {
        "id": "lph809FEAcpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2차원으로 축소하는 t-SNE 모델을 생성합니다.\n",
        "tsne_2d_model = TSNE(perplexity=15,n_components=2, n_iter=3600, random_state=0)"
      ],
      "metadata": {
        "id": "2AzOB3V9ip5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `perplexity` : 학습에 영향을 주는 점들의 개수를 조절합니다.    \n",
        "\n",
        "    보통 5~50사이의 값을 사용하며, 값이 작을 수록 global structure 보다 local structure에 더 집중합니다.\n",
        "* `n_compontnets` : 임베딩 공간의 차원\n",
        "* `n_iter` : 최적화를 위한 최대 반복 횟수입니다. 최소한 250 이상은 되어야 합니다.\n",
        "* `random_state` : 난수(random number) 생성 알고리즘에서 사용하는 seed(씨앗)을 설정합니다.  \n",
        "* 파라미터에 대한 자세한 설명은 [여기](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)를 참조하세요."
      ],
      "metadata": {
        "id": "aodTcK9qIN4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tsne_2d(title, tsne, X):\n",
        "    # 100차원에서 2차원으로 임베딩 결과를 축소합니다.\n",
        "    X_tsne = tsne.fit_transform(X[:300,:])\n",
        "    # 각 단어별 x좌표와 y좌표를 Dataframe으로 저장합니다.\n",
        "    df = pd.DataFrame(X_tsne, index=vocab[:300], columns=['x', 'y'])\n",
        "    \n",
        "    # 그래프를 생성하고 출력합니다.\n",
        "    %matplotlib inline              \n",
        "    fig = plt.figure()              # 그래프 생성\n",
        "    fig.set_size_inches(20, 10)     # 그래프 사이즈 설정\n",
        "    ax = fig.add_subplot(1, 1, 1)   # 2D 축 생성\n",
        "    ax.scatter(df[\"x\"], df[\"y\"])    # 각 좌표에 점 표시\n",
        "    for word, pos in list(df.iterrows()):\n",
        "        ax.annotate(word, pos, fontsize=12, fontproperties=prop) # 단어 주석\n",
        "    plt.title(title)                # 제목 표시\n",
        "    plt.show()                      # 그래프출력"
      ],
      "metadata": {
        "id": "YTQ2urDv_jB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_2d('Visualizing Embeddings using t-SNE', tsne_2d_model, X)"
      ],
      "metadata": {
        "id": "767eQ6JKAMh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) tsne 3차원 축소"
      ],
      "metadata": {
        "id": "EoQq3JVTAUGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.cm as cm"
      ],
      "metadata": {
        "id": "rVJ3bWiFXkDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3차원으로 축소하는 t-SNE 모델을 만들어줍니다.\n",
        "tsne_3d_model = TSNE(perplexity=15, n_components=3, n_iter=3500, random_state=0)"
      ],
      "metadata": {
        "id": "PbFdu8AGYpO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tsne_3d(title, tsne, X, a=1):\n",
        "    # 100차원에서 3차원으로 임베딩 결과를 축소합니다.\n",
        "    X_tsne = tsne.fit_transform(X)\n",
        "    # 각 단어별 x,y,z좌표를 Dataframe으로 저장합니다.\n",
        "    df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y', 'z'])\n",
        "\n",
        "    # 그래프를 생성하고 출력합니다.\n",
        "    %matplotlib inline\n",
        "    fig = plt.figure()                      # 그래프 이미지 생성\n",
        "    ax = fig.add_subplot(projection='3d')   # 3D 축 생성\n",
        "    ax.scatter(df[\"x\"], df[\"y\"], df[\"z\"], c='crimson', alpha=a) # 각 좌표에 점 표시\n",
        "    plt.title(title)        # 제목 표시\n",
        "    plt.show()              # 그래프 출력"
      ],
      "metadata": {
        "id": "0YFTdTgGgL57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne_3d('Visualizing Embeddings using t-SNE', tsne_3d_model, X, a=0.2)"
      ],
      "metadata": {
        "id": "dqAC2sXTAm0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👀[참고] 특정 단어의 3차원 시각화"
      ],
      "metadata": {
        "id": "klIzyj7DhPTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tsne_3d_sample(title, words, tsne, a=1):    \n",
        "    # 100차원에서 3차원으로 임베딩 결과를 축소합니다.\n",
        "    X_tsne = tsne.fit_transform(X)\n",
        "    # 각 단어별 x,y,z좌표를 Dataframe으로 저장합니다.\n",
        "    df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y', 'z'])\n",
        "    # 그래프를 생성하고 출력합니다.\n",
        "    %matplotlib inline\n",
        "    fig = plt.figure()                      # 그래프 이미지 생성\n",
        "    ax = fig.add_subplot(projection='3d')   # 3D 축 생성\n",
        "    for word in words:                      # 샘플 단어 좌표 및 주석 표시\n",
        "        x = df[\"x\"][word]\n",
        "        y = df[\"y\"][word]\n",
        "        z = df[\"z\"][word]\n",
        "        ax.scatter(x, y, z, c='crimson', alpha=a)               \n",
        "        ax.text(x, y, z, word, fontsize=10, zorder=1, fontproperties=prop)    \n",
        "    plt.title(title)    # 제목 표시\n",
        "    plt.show()          # 그래프 출력"
      ],
      "metadata": {
        "id": "8xHWbg7fdNar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_words=['지구','독일', '영국', '미국']\n",
        "tsne_3d_sample('Visualizing Embeddings using t-SNE', sample_words, tsne_3d_model, a=1)"
      ],
      "metadata": {
        "id": "vNRPqyRueEmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 👀 [참고] Word Embedding Test\n",
        "* 자연어 단어 간 품사적, 의미론적 관계가 임베딩에 얼마나 잘 녹아 있는지 정량적으로 평가해봅니다.\n",
        "\n",
        "* 단어 유사도 평가(Word similarity test)\n",
        "  * 일련의 단어쌍을 미리 구성한 후에 사람이 평가한 점수와 단어 벡터 간 코사인 유사도 사이의 상관관계(correlation)를 계산해 단어 임베딩의 품질을 평가하는 방법\n",
        "\n",
        "* 단어 유추 평가(Word analogy test)\n",
        "  * 단어벡터간 계산을 통해 질의에 대한 정답을 도출해낼 수 있는지 평가\n",
        "*  데이터와 코드 참고 :[이동준님의 github](https://github.com/dongjun-Lee/kor2vec)"
      ],
      "metadata": {
        "id": "C204evEaAyR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import scipy.stats as st"
      ],
      "metadata": {
        "id": "yCwjyWdxoO67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) 단어 유사도 평가"
      ],
      "metadata": {
        "id": "tLgCH_vyCvEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#평가 데이터의 사람이 평가한 유사도 점수와, 모델의 임베딩벡터 쌍 간 코사인 유사도의 상관관계를 반환합니다.\n",
        "def word_sim_test(test_fname, wordvectors):\n",
        "        actual_sim_list, pred_sim_list = [], []\n",
        "        missed = 0\n",
        "        with open(test_fname, 'r') as pairs:\n",
        "            for pair in pairs:\n",
        "                w1, w2, actual_sim = pair.strip().split(\",\")\n",
        "                try:\n",
        "                    pred_sim = wordvectors.similarity(w1, w2)         # 모델의 임베딩 벡터 쌍 간 코사인 유사도\n",
        "                    actual_sim_list.append(float(actual_sim))         # 사람이 평가한 유사도 점수수\n",
        "                    pred_sim_list.append(pred_sim)\n",
        "                except KeyError:\n",
        "                    missed += 1\n",
        "                    \n",
        "        spearman, _ = st.spearmanr(actual_sim_list, pred_sim_list) # 스피어만 상관계수\n",
        "        pearson, _ = st.pearsonr(actual_sim_list, pred_sim_list)   # 피어슨 상관계수\n",
        "        return spearman, pearson, missed"
      ],
      "metadata": {
        "id": "V6DB8TiPCxjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트셋의 경로입니다.\n",
        "test_fname = \"/content/kor_ws353.csv\""
      ],
      "metadata": {
        "id": "9SvB-zYeD2yI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위에서 학습했던 Word2Vec과 FastText의 임베딩 결과(워드벡터)를 불러옵니다.\n",
        "word2vec_wordvectors = KeyedVectors.load('word2vec.wordvectors')\n",
        "fasttext_wordvectors = KeyedVectors.load('fasttext.wordvectors')"
      ],
      "metadata": {
        "id": "zzdjhtdg-Mvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 모델의 단어 유사도 평가를 수행합니다.\n",
        "word2vec_test = word_sim_test(test_fname, word2vec_wordvectors)\n",
        "fasttext_test = word_sim_test(test_fname, fasttext_wordvectors)"
      ],
      "metadata": {
        "id": "0Os0AmhSyGms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 상관계수가 1에 가까울 수록 둘 사이의 상관관계가 강하다는 뜻입니다.\n",
        "# (spearman, pearson, missed)\n",
        "print(word2vec_test)\n",
        "print(fasttext_test)"
      ],
      "metadata": {
        "id": "Q0oo7zkUyMtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유사도 평가 결과를 데이터프레임으로 출력합니다.\n",
        "df = pd.DataFrame({\"Word Embedding\": [\"Word2Vec\",\"Word2Vec\",\"FastText\",\"FastText\"]})\n",
        "df[\"Criterion\"] = [\"spearman\", \"pearson\"]*2\n",
        "df[\"score\"] = list(word2vec_test[:-1] + fasttext_test[:-1])\n",
        "df"
      ],
      "metadata": {
        "id": "xyQiIhxPD4KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유사도 평가 결과를 bar plot으로 출력합니다.\n",
        "sns.barplot(x=\"Word Embedding\", y=\"score\", hue=\"Criterion\", data=df)\n",
        "plt.title(\"Word Similarity Test Result\")"
      ],
      "metadata": {
        "id": "42I5dgg7D9es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) 단어 유추 평가"
      ],
      "metadata": {
        "id": "emSkSHkaEBRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 벡터 간 계산의 결과가 의미론적 유추에서의 정답을 도출해낼 수 있는지 평가합니다.\n",
        "def word_analogy_test(test_fname, wordvectors):\n",
        "        correct, total, missed = 0, 0, 0\n",
        "\n",
        "        with open(test_fname, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"#\") or len(line) <= 1: continue\n",
        "                words = line.strip().split(\" \")\n",
        "\n",
        "                try:\n",
        "                    predicted_answer = [i[0] for i in wordvectors.most_similar(positive=[words[0], words[2]], negative= [words[1]], topn=30)]\n",
        "                    # print(words[0] + \" - \" + words[1] + \" + \" + words[2])\n",
        "                    # print(\"correct answer:\", words[3])\n",
        "                    # print(\"predicted answers:\", predicted_answer[0])\n",
        "                    # print(\"\")\n",
        "                    if words[-1] in predicted_answer: correct += 1\n",
        "                except:\n",
        "                    missed += 1\n",
        "                \n",
        "                total += 1\n",
        "        print(wordvectors)\n",
        "        print(\"# of correct answer:\", correct, \", # of data:\", total, \", # of errors:\", missed)\n",
        "        print()\n",
        "        return correct/(total-missed) # 맞춘 개수 / 처리 데이터 수"
      ],
      "metadata": {
        "id": "tl8-2Bi6EE1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_fname = \"/content/kor_analogy_semantic.txt\""
      ],
      "metadata": {
        "id": "24JFpT1-EGoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 모델의 단어 유추 평가를 수행합니다.\n",
        "word2vec_test = word_analogy_test(test_fname, word2vec_wordvectors)\n",
        "fasttext_test = word_analogy_test(test_fname, fasttext_wordvectors)"
      ],
      "metadata": {
        "id": "_QWtnRzB2OnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유추 평가 결과를 데이터프레임으로 출력합니다.\n",
        "df2 = pd.DataFrame({\"Word Embedding\": [\"Word2Vec\",\"FastText\"]})\n",
        "df2[\"score\"] = [word2vec_test, fasttext_test]\n",
        "df2"
      ],
      "metadata": {
        "id": "uYnve393EH0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 유사도 평가 결과를 bar plot으로 출력합니다.\n",
        "sns.barplot(x=\"Word Embedding\", y=\"score\", data=df2)\n",
        "plt.title(\"Word Analogy Test Result\")"
      ],
      "metadata": {
        "id": "uM7Wbdz5EKE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-hIVmlJodEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "26b513cc33e7b0f32234e00c3eeccc0fe2bd714cda17604d54a97c263bf3993c"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}