{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11x0Woz0f5O67EqiAmdOhGdZJJnCOszbl","timestamp":1675508264360},{"file_id":"1UeQcWx8njHIuRhwaPOAhLjHcacwaei_3","timestamp":1675492219247}],"authorship_tag":"ABX9TyMsr1MB+D8fkZVJQC/C9zjf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<!-- ---\n","- Project: 2023 Winter School\n","- Author: Gyu-min Lee\n","- Version: 0.10\n","- Changelog\n","    - 0.1 -- Initiated the file\n","    - 0.5 -- First Draft\n","    - 0.9 -- Proofread\n","    - 0.10 -- Restructured with Tensorboard integration\n","--- -->\n","\n","2023 전산언어학 겨울학교 5일차 2교시\n","\n","# Syntax "],"metadata":{"id":"sKVGZDums7tq"}},{"cell_type":"markdown","source":["## Project: SNLI with ALBERT \n","\n","- 🤗 Hub의 모델과 데이터셋을 불러와 파인튜닝 및 성능 평가를 진행합니다\n","- 📔NOTE: 빠른 실행을 위해 Runtime 유형을 'GPU'로 해 주세요"],"metadata":{"id":"tBXi4nFXH3se"}},{"cell_type":"code","source":["!pip install transformers==4.26.0 datasets==2.9.0"],"metadata":{"id":"kp5yCyN_H3c2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP0: Load libraries\n","import random\n","\n","import torch\n","\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","import datasets\n","\n","from transformers import pipeline\n","from transformers import Trainer, TrainingArguments\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification\n","\n","from tqdm import tqdm"],"metadata":{"id":"PAqjQ3okIipN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP1: Prepare data\n","DATASET_NAME = \"snli\"\n","# https://huggingface.co/datasets/snli\n","\n","dataset = datasets.load_dataset(DATASET_NAME)\n","dataset"],"metadata":{"id":"GALIQE2T8Jnr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP2: Prepare models\n","MODEL_NAME = \"albert-base-v2\"\n","# https://huggingface.co/albert-base-v2\n","\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,\n","                                           num_labels = 3)\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"],"metadata":{"id":"hZZFdn97KF_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(dataset):\n","    text = dataset['premise'] + ' ' + dataset['hypothesis']\n","    result = text.lower()\n","    result = tokenizer(result, truncation=True)\n","\n","    return result "],"metadata":{"id":"Q_YWWgBeVAwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'] = dataset['train'].select(random.sample(range(len(dataset['train'])), 7000))\n","dataset['test'] = dataset['test'].select(random.sample(range(len(dataset['test'])), 2000))\n","dataset['validation'] = dataset['validation'].select(random.sample(range(len(dataset['validation'])), 1000))\n","# sampling the data for demonstration purpose only"],"metadata":{"id":"ucFCcJV6adNR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'] = dataset['train'].map(preprocess)\n","dataset['test'] = dataset['test'].map(preprocess)\n","dataset['validation'] = dataset['validation'].map(preprocess)"],"metadata":{"id":"mAVhWSLtWBQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'] = dataset['train'].rename_column(\"label\", \"labels\")\n","dataset['test'] = dataset['test'].rename_column(\"label\", \"labels\")\n","dataset['validation'] = dataset['validation'].rename_column(\"label\", \"labels\")\n","\n","\n","dataset['train'] = dataset['train'].filter(lambda x: x['labels'] != -1) \n","dataset['test'] = dataset['test'].filter(lambda x: x['labels'] != -1)\n","dataset['validation'] = dataset['validation'].filter(lambda x: x['labels'] != -1)"],"metadata":{"id":"2HOYZBjzb9jR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP3: Set up a trainer\n","\n","def metrics(model_output) -> dict:\n","    labels = model_output.label_ids\n","    predictions = model_output.predictions.argmax(-1)\n","\n","    accuracy = accuracy_score(labels, predictions)\n","    f1 = f1_score(labels, predictions, average='weighted')\n","\n","    return {\"accuracy\": accuracy,\n","            \"f1\": f1}\n","            \n","training_args = TrainingArguments(num_train_epochs=2,\n","                                    # probably increase epochs for better result\n","                                    output_dir='./checkpoints',\n","                                    per_device_train_batch_size=64,\n","                                    per_device_eval_batch_size=128,\n","                                    evaluation_strategy='steps', # or, 'epoch'\n","                                    logging_dir='./checkpoints/logs',\n","                                    logging_steps=50,\n","                                    save_steps=100,\n","                                    load_best_model_at_end=True,\n","                                    )\n","\n","trainer = Trainer(model=model,\n","                    args=training_args,\n","                    train_dataset=dataset[\"train\"], \n","                    eval_dataset=dataset[\"validation\"],\n","                    tokenizer=tokenizer,\n","                    compute_metrics=metrics,\n","                )\n","                          "],"metadata":{"id":"6yg3b4HwbKzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.is_available()"],"metadata":{"id":"RNQoBM1kU_DI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard\n","%tensorboard --logdir ./checkpoints/logs"],"metadata":{"id":"iLwz0WpN7VTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP4: train\n","trainer.train()"],"metadata":{"id":"_RxpzkImcRda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP5: predict\n","DEVICE = 'cuda:0' if torch.cuda.is_available else 'cpu'\n","\n","pipe = pipeline(task='text-classification',\n","                model=model,\n","                tokenizer=tokenizer,\n","                device=DEVICE,\n","                )\n","\n","test_inputs =  [data['premise'] + ' ' + data['hypothesis'] for data in dataset['test']]\n","test_labels = [data['labels'] for data in dataset['test']]\n","\n","preds = pipe(test_inputs)\n"],"metadata":{"id":"BkimZhN-cSst"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds[:10]"],"metadata":{"id":"TrWpumeUfIkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["counter = 0\n","\n","for input, label, pred in zip(test_inputs, test_labels, preds): \n","    print(input) \n","    print(label, end='\\t')\n","    print(pred['label'])\n","    print('\\n')\n","    counter += 1\n","    if counter >= 10:\n","        break"],"metadata":{"id":"VKRzqJFm9WOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset['train'].features['labels']"],"metadata":{"id":"JzK7Dayv8_yY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = [0 if pred['label'] == \"LABEL_0\" \n","         else 1 if pred['label'] == \"LABEL_1\" else 2\n","         for pred in preds]\n"],"metadata":{"id":"jCmOJdMsfKI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accruacy = accuracy_score(test_labels, preds)\n","f1 = f1_score(test_labels, preds, average='weighted')\n","\n","print(f\"Accuracy: \\t{accruacy:04.2f}\")\n","print(f\"F1: \\t{f1:04.2f}\")"],"metadata":{"id":"g-l85mW8fx6x"},"execution_count":null,"outputs":[]}]}