{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ---\n",
        "- Project: 2023 Winter School\n",
        "- Author: Gyu-min Lee\n",
        "- Version: 0.5\n",
        "- Changelog\n",
        "    - 0.1 -- Initiated the file\n",
        "    - 0.5 -- First Draft\n",
        "- Todo\n",
        "    = [ ] Spellcheck and proofread\n",
        "--- -->"
      ],
      "metadata": {
        "id": "Q9gzdxqAsnsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2023 전산언어학 겨울학교 3일차 1교시\n",
        "\n",
        "# NLP Libraries"
      ],
      "metadata": {
        "id": "sKVGZDums7tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [SLIDE 8] 멀티파일 프로그래밍"
      ],
      "metadata": {
        "id": "0ujIm5yCtQUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone $GIT_ADDRESS\n",
        "# %cd $SESSION_PATH"
      ],
      "metadata": {
        "id": "8btdVfZECQYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-1-1\n",
        "\n",
        "import sample1\n",
        "\n",
        "sample1.print_1()"
      ],
      "metadata": {
        "id": "kpqCpM2XtTxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-1-2\n",
        "\n",
        "import sample1\n",
        "\n",
        "from sample2 import print_2\n",
        "\n",
        "sample1.print_1()\n",
        "print_2()"
      ],
      "metadata": {
        "id": "2gEhSS4fCbzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-1-3\n",
        "\n",
        "import sample1\n",
        "\n",
        "from sample2 import print_2\n",
        "\n",
        "sample1.print_1()\n",
        "print_2()\n",
        "\n",
        "run_me()\n",
        "# DO: fix line 5 to run line 10"
      ],
      "metadata": {
        "id": "BtaXk8xNCjhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-1-4\n",
        "\n",
        "import sample1 as sample\n",
        "\n",
        "from sample2 import print_2 as mario\n",
        "from sample2 import run_me as luigi\n",
        "\n",
        "sample.print_1()\n",
        "\n",
        "mario()\n",
        "luigi()"
      ],
      "metadata": {
        "id": "zYBk_LiVCpUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-1-5\n",
        "\n",
        "from sample2 import *\n",
        "\n",
        "print_2()\n",
        "run_me()"
      ],
      "metadata": {
        "id": "-9ySPtX8DARN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [SLIDE 12] 라이브러리"
      ],
      "metadata": {
        "id": "SCJ7Hi1uHWXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-2-1\n",
        "\n",
        "import random\n",
        "\n",
        "random.randint(0, 100)"
      ],
      "metadata": {
        "id": "G8ofsVqRGas8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-2-2\n",
        "\n",
        "import random\n",
        "\n",
        "random.randint?"
      ],
      "metadata": {
        "id": "Jvvg7srdHgEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-2-3\n",
        "\n",
        "# https://pypi.org/project/pyjokes/\n",
        "\n",
        "!pip install pyjokes\n",
        "\n",
        "from pyjokes import get_joke\n",
        "\n",
        "print(\"A joke from pyjokes...\")\n",
        "print('='*30)\n",
        "\n",
        "get_joke()"
      ],
      "metadata": {
        "id": "gmTL8vrqHnKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-2-4\n",
        "\n",
        "!pip show pyjokes "
      ],
      "metadata": {
        "id": "Jt-uhZMyI-xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-2-5\n",
        "\n",
        "# https://pypi.org/project/pyjokes/#history\n",
        "\n",
        "!pip install pyjokes==0.5.0\n",
        "\n",
        "!pip show pyjokes"
      ],
      "metadata": {
        "id": "pA_NqklqJItP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-2-6\n",
        "\n",
        "!pip freeze\n",
        "\n",
        "# !pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "_lse5PJlJosG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-2-7\n",
        "\n",
        "!pip install -r 2023winter_3_1_requirements.txt"
      ],
      "metadata": {
        "id": "s8gkwRrhJ3sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [SLIDE 19] NLTK를 통한 자연어처리 기술"
      ],
      "metadata": {
        "id": "esmn61iZdLj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install nltk"
      ],
      "metadata": {
        "id": "Ju2yJgy5v95M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-1\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "txt = \"\"\"The printer was broken, and no one could \n",
        "figure out whose fault it was. After arguing back \n",
        "and forth, our supervisor took charge. “Look,” he said, \n",
        "“we really don’t need to determine who is responsible for \n",
        "this mess. We just want someone to take the blame.\"\"\"\n",
        "# text here and below retreived from: https://www.rd.com/jokes/office/\n",
        "\n",
        "word_tokenize(txt)\n",
        "# DO: add two lines at least before line 11 to fix the code\n",
        "# **hint** The answer is in the answer :)\n"
      ],
      "metadata": {
        "id": "nDmCaZeGWK8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-2\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "txt = \"\"\"The printer was broken, and no one could \n",
        "figure out whose fault it was. After arguing back \n",
        "and forth, our supervisor took charge. “Look,” he said, \n",
        "“we really don’t need to determine who is responsible for \n",
        "this mess. We just want someone to take the blame.\"\"\"\n",
        "\n",
        "sents = sent_tokenize(txt)\n",
        "print(\"sent_tokenize result:\")\n",
        "print(sents)\n",
        "\n",
        "print('='*30)\n",
        "\n",
        "sent_tokens = list()\n",
        "for sent in sents:\n",
        "    sent_tokens.append(word_tokenize(sent))\n",
        "\n",
        "print(\"sentences are now tokenized as:\")\n",
        "for sent_token in sent_tokens:\n",
        "    print(sent_token)"
      ],
      "metadata": {
        "id": "7CDQxecUeYpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-3\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "token_1 = \"worked\"\n",
        "token_1_lemma = lemmatizer.lemmatize(token_1, pos='v')\n",
        "token_1_stem = stemmer.stem(token_1)\n",
        "\n",
        "token_2 = \"went\"\n",
        "token_2_lemma = lemmatizer.lemmatize(token_2, pos='v')\n",
        "token_2_stem = stemmer.stem(token_2)\n",
        "\n",
        "token_3 = \"studied\"\n",
        "token_3_lemma = lemmatizer.lemmatize(token_3, pos='v')\n",
        "token_3_stem = stemmer.stem(token_3)\n",
        "\n",
        "print('='*30)\n",
        "print(\"RESULTS\")\n",
        "\n",
        "print(f\"\"\"Result for: {token_1}\n",
        "\\tlemma: {token_1_lemma}\n",
        "\\tstem: {token_1_stem}\n",
        "\n",
        "Result for: {token_2}\n",
        "\\tlemma: {token_2_lemma}\n",
        "\\tstem: {token_2_stem}\n",
        "\n",
        "Result for: {token_3}\n",
        "\\tlemma: {token_3_lemma}\n",
        "\\tstem: {token_3_stem}\"\"\")\n",
        "print('='*30)"
      ],
      "metadata": {
        "id": "S_0ayB5HfCnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-4\n",
        "\n",
        "import re\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# read the text\n",
        "with open('Doyle_1892_excerpt_adventure2.txt') as f:\n",
        "    content = f.read() \n",
        "\n",
        "# preprocess the text\n",
        "content = re.sub(r'\\[ADVENTURE II .*?\\]\\n\\n', '', content)\n",
        "content = content.lower()\n",
        "\n",
        "# tokenize\n",
        "tokens = word_tokenize(content)\n",
        "print(\"Checking the tokenizations...\")\n",
        "print(\"Original:\")\n",
        "print(content[:300])\n",
        "print(\"First 100 tokens:\")\n",
        "print(tokens[:100])\n",
        "\n",
        "# frequencies\n",
        "size = len(tokens)\n",
        "frequency_holmes = tokens.count('holmes')\n",
        "frequency_watson = tokens.count('watson')\n",
        "\n",
        "print(\"Calculating frequencies...\")\n",
        "print(f\"Size: {size:,}\")\n",
        "print(f\"Abolute frequency of 'holmes': {frequency_holmes}\")\n",
        "print(f\"Abolute frequency of 'watson': {frequency_watson}\")\n",
        "\n",
        "frequency_holmes_permill = frequency_holmes * 1000000 / size\n",
        "frequency_watson_permill = frequency_watson * 1000000 / size\n",
        "print(f\"Per-mill relative frequency of 'holmes': {frequency_holmes_permill:7,.2f}\")\n",
        "print(f\"Per-mill relative frequency of 'watson': {frequency_watson_permill:7,.2f}\")"
      ],
      "metadata": {
        "id": "QQP_AEG-f2F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-5\n",
        "\n",
        "import re\n",
        "\n",
        "from nltk import FreqDist\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# read the text\n",
        "with open('Doyle_1892_excerpt_adventure2.txt') as f:\n",
        "    content = f.read() \n",
        "\n",
        "# preprocess the text\n",
        "content = re.sub(r'\\[ADVENTURE II .*?\\]\\n\\n', '', content)\n",
        "content = content.lower()\n",
        "tokens = word_tokenize(content)\n",
        "\n",
        "freq_table = FreqDist(tokens)\n",
        "\n",
        "freq_table.most_common(30)"
      ],
      "metadata": {
        "id": "b3IE_BQXhAnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-6\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "\n",
        "from nltk import FreqDist\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "# read the text\n",
        "with open('Doyle_1892_excerpt_adventure2.txt') as f:\n",
        "    content = f.read() \n",
        "\n",
        "# preprocess the text\n",
        "content = re.sub(r'\\[ADVENTURE II .*?\\]\\n\\n', '', content)\n",
        "content = content.lower()\n",
        "tokens = word_tokenize(content)\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "punctuations = [',', '.', '``', '\\'', '\\'\\'', '\\\"', '\\\"\\\"', '?', '!',\n",
        "                '-', '--', ':', ';']\n",
        "\n",
        "cleaned_tokens = list()\n",
        "for token in tokens:\n",
        "    if token in stopwords or token in punctuations:\n",
        "        pass\n",
        "    else:\n",
        "        cleaned_tokens.append(token)\n",
        "\n",
        "freq_table = FreqDist(cleaned_tokens)\n",
        "\n",
        "freq_table.most_common(100)"
      ],
      "metadata": {
        "id": "_r9iqe9Gks6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-7\n",
        "\n",
        "import re\n",
        "\n",
        "from nltk import FreqDist\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# read the text\n",
        "with open('Doyle_1892_excerpt_adventure2.txt') as f:\n",
        "    content = f.read() \n",
        "\n",
        "# preprocess the text\n",
        "content = re.sub(r'\\[ADVENTURE II .*?\\]\\n\\n', '', content)\n",
        "content = content.lower()\n",
        "tokens = word_tokenize(content)\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "punctuations = [',', '.', '``', '\\'', '\\'\\'', '\\\"', '\\\"\\\"', '?', '!',\n",
        "                '-', '--', ':', ';']\n",
        "\n",
        "cleaned_tokens = list()\n",
        "for token in tokens:\n",
        "    if token in stopwords or token in punctuations:\n",
        "        pass\n",
        "    else:\n",
        "        cleaned_tokens.append(token)\n",
        "\n",
        "freq_table = FreqDist(cleaned_tokens)\n",
        "\n",
        "freq_table_list = freq_table.most_common()\n",
        "\n",
        "with open('out_freqs.txt', 'w') as f:\n",
        "    for item in freq_table_list:\n",
        "        f.write(f\"{item[0]}\\t{item[1]}\\n\")"
      ],
      "metadata": {
        "id": "J8_hUrGjlQm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-8\n",
        "\n",
        "import re\n",
        "\n",
        "from nltk import Text\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# read the text\n",
        "with open('Doyle_1892_excerpt_adventure2.txt') as f:\n",
        "    content = f.read() \n",
        "\n",
        "# preprocess the text\n",
        "content = re.sub(r'\\[ADVENTURE II .*?\\]\\n\\n', '', content)\n",
        "tokens = word_tokenize(content)\n",
        "\n",
        "text = Text(tokens)\n",
        "\n",
        "text.concordance('Holmes')\n",
        "# text.concordance('Holmes', lines=53)"
      ],
      "metadata": {
        "id": "aqQZOol3nGaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-3-8\n",
        "\n",
        "import re\n",
        "\n",
        "from nltk import Text\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# read the text\n",
        "with open('Doyle_1892_excerpt_adventure2.txt') as f:\n",
        "    content = f.read() \n",
        "\n",
        "# preprocess the text\n",
        "content = re.sub(r'\\[ADVENTURE II .*?\\]\\n\\n', '', content)\n",
        "tokens = word_tokenize(content)\n",
        "\n",
        "text = Text(tokens)\n",
        "\n",
        "holmes_kwics = text.concordance_list('Holmes', lines=None)\n",
        "\n",
        "print(\"Each KWiC looks like...\")\n",
        "print(holmes_kwics[0])\n",
        "\n",
        "with open('out_holmes.txt', 'w') as f:\n",
        "    for kwic in holmes_kwics:\n",
        "        left_whole = ' '.join(kwic.left)\n",
        "        middle = kwic.query\n",
        "        middle = f\"**{middle}**\"\n",
        "        right_whole = ' '.join(kwic.right)\n",
        "        line = ' '.join([left_whole, middle, right_whole])\n",
        "        f.write(line)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "YZuXEyxOnmTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [SLIDE 23] KoNLPy를 활용한 한국어 토큰화"
      ],
      "metadata": {
        "id": "QiJYiw6Fpno5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install konlpy"
      ],
      "metadata": {
        "id": "vjpNVj5as5-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-4-1\n",
        "\n",
        "from konlpy.tag import Kkma # Hannanum, Komoran, Mecab, Okt\n",
        "# additional installation required for Mecab\n",
        "# To use Mecab, Run:\n",
        "# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "\n",
        "text = \"\"\"많은 사람들이 자기를 과신하여 미혹에 빠졌고 \n",
        "그 망상 떄문에 정도에서 벗어났다. 눈이 멀고서야 어찌 \n",
        "빛을 보랴? 자신도 모르면서 남을 설득하려 들지 말아라.\"\"\"\n",
        "# 공동번역성서 발췌\n",
        "\n",
        "tagger = Kkma()\n",
        "\n",
        "print(\"분석 대상 텍스트:\")\n",
        "print(text)\n",
        "print(\"명사들은...\")\n",
        "print(tagger.nouns(text))\n",
        "print(\"형태소들은...\")\n",
        "print(tagger.morphs(text))\n",
        "print(\"형태소 + 품사는...\")\n",
        "print(tagger.pos(text))"
      ],
      "metadata": {
        "id": "06_WWhGYn7gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-4-2\n",
        "\n",
        "from konlpy.tag import Kkma \n",
        "\n",
        "text = \"\"\"많은 사람들이 자기를 과신하여 미혹에 빠졌고 \n",
        "그 망상 떄문에 정도에서 벗어났다. 눈이 멀고서야 어찌 \n",
        "빛을 보랴? 자신도 모르면서 남을 설득하려 들지 말아라.\"\"\"\n",
        "# 공동번역성서 발췌\n",
        "\n",
        "tagger = Kkma()\n",
        "\n",
        "tagged = tagger.pos(text)\n",
        "\n",
        "verbs = list()\n",
        "for token in tagged: \n",
        "    if token[1].startswith('J'):\n",
        "        verbs.append(token[0])\n",
        "\n",
        "print(\"조사만 추리면...\")\n",
        "print(verbs)"
      ],
      "metadata": {
        "id": "lM-EF-TLruos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-4-3\n",
        "\n",
        "from konlpy.tag import Kkma \n",
        "\n",
        "text = \"\"\"많은 사람들이 자기를 과신하여 미혹에 빠졌고 \n",
        "그 망상 떄문에 정도에서 벗어났다. 눈이 멀고서야 어찌 \n",
        "빛을 보랴? 자신도 모르면서 남을 설득하려 들지 말아라.\"\"\"\n",
        "# 공동번역성서 발췌\n",
        "\n",
        "tagger = Kkma()\n",
        "\n",
        "tagged = tagger.pos(text)\n",
        "\n",
        "tokens = list()\n",
        "for token in tagged:\n",
        "    tokens.append(f\"<{token[0]}:{token[1]}>\")\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "aPKWojzQsOye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [SLIDE 27] spaCy 등등"
      ],
      "metadata": {
        "id": "bqL4cEjHvrtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spacy"
      ],
      "metadata": {
        "id": "Os7r2md6uT8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-5-1\n",
        "\n",
        "import spacy\n",
        "\n",
        "txt = \"\"\"The printer was broken, and no one could \n",
        "figure out whose fault it was. After arguing back \n",
        "and forth, our supervisor took charge. “Look,” he said, \n",
        "“we really don’t need to determine who is responsible for \n",
        "this mess. We just want someone to take the blame.\"\"\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# https://spacy.io/models\n",
        "\n",
        "tokens = nlp(txt)\n",
        "\n",
        "print(\"A single 'token' contains:\")\n",
        "print(tokens[0].text)\n",
        "print(tokens[0].lemma_)\n",
        "print(tokens[0].pos_)\n",
        "print(tokens[0].dep_)\n",
        "print(tokens[0].shape_)\n",
        "print(tokens[0].is_stop)"
      ],
      "metadata": {
        "id": "X2jzrXn-v8za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-5-2\n",
        "\n",
        "import spacy\n",
        "\n",
        "txt = \"\"\"The printer was broken, and no one could \n",
        "figure out whose fault it was. After arguing back \n",
        "and forth, our supervisor took charge. “Look,” he said, \n",
        "“we really don’t need to determine who is responsible for \n",
        "this mess. We just want someone to take the blame.\"\"\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# https://spacy.io/models\n",
        "\n",
        "tokens = nlp(txt)\n",
        "\n",
        "lemmas = list()\n",
        "for token in tokens:\n",
        "    if token.is_stop == False:\n",
        "        if token.pos_ != \"PUNCT\" and token.text != '\\n':\n",
        "            # spacy.explain('PUNCT')\n",
        "            lemmas.append(token.lemma_)\n",
        "print(lemmas)"
      ],
      "metadata": {
        "id": "kIvVtoykweBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-5-3\n",
        "\n",
        "import spacy\n",
        "\n",
        "txt = \"日々普通の日常の中で宝物の探しができる人になりたいと、田中は思った。\"\n",
        "# \"매일 평범한 일상 속에서 보물을 찾을 수 있는 사람이 되고 싶다고 다나카는 생각했다.\"\n",
        "\n",
        "!python -m spacy download ja_core_news_sm\n",
        "nlp = spacy.load('ja_core_news_sm')\n",
        "# https://spacy.io/models\n",
        "\n",
        "tokens = nlp(txt)\n",
        "\n",
        "morphs = list()\n",
        "for token in tokens:\n",
        "    morphs.append(f\"{token.text}/{token.pos_}\")\n",
        "\n",
        "print(morphs)"
      ],
      "metadata": {
        "id": "EA-FWh35w0m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-5-4\n",
        "\n",
        "import spacy\n",
        "\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "from itertools import zip_longest \n",
        "\n",
        "text = \"매일 평범한 일상 속에서 보물을 찾을 수 있는 사람이 되고 싶다고 영희는 생각했다.\"\n",
        "\n",
        "tagger = Kkma() \n",
        "tagged_kkma = tagger.pos(text)\n",
        "\n",
        "!python -m spacy download ko_core_news_lg\n",
        "nlp = spacy.load('ko_core_news_lg')\n",
        "tokens = nlp(text)\n",
        "tagged_spacy = [(token.text, token.pos_) for token in tokens]\n",
        "\n",
        "print(\"꼬꼬마_형태/꼬꼬마_태그\\tspaCy_형태/spaCy_태그\")\n",
        "for token_kkma, token_spacy in zip_longest(tagged_kkma, tagged_spacy):\n",
        "    try:\n",
        "        print(f\"{token_kkma[0]}/{token_kkma[1]}\\t\\t{token_spacy[0]}/{token_spacy[1]}\")\n",
        "    except TypeError:\n",
        "        if len(tagged_kkma) > len(tagged_spacy):\n",
        "            print(f\"{token_kkma[0]}/{token_kkma[1]}\\t\\t-/-\")\n",
        "        else:\n",
        "            print(f\"-/-\\t\\t{token_spacy[0]}/{token_spacy[1]}\")\n"
      ],
      "metadata": {
        "id": "E4O462mOzmLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-5-5\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "txt = \"The horse raced past the barn.\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# https://spacy.io/models\n",
        "\n",
        "tokens = nlp(txt)\n",
        "\n",
        "displacy.render(tokens, style='dep', jupyter=True)"
      ],
      "metadata": {
        "id": "B6-ehxHT6NPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-1-5-6\n",
        "\n",
        "import re \n",
        "\n",
        "from nltk import FreqDist\n",
        "from nltk import Text\n",
        "\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "tagger = Kkma() \n",
        "\n",
        "with open('이효석_1936_발췌.txt') as f:\n",
        "    content = f.read()\n",
        "\n",
        "content = re.sub(r'메밀꽃 필 무렵.*?\\n', '', content)\n",
        "content = re.sub('\\n', ' ', content)\n",
        "\n",
        "tagged = tagger.pos(content)\n",
        "\n",
        "tokens = list()\n",
        "for token_tagged in tagged:\n",
        "    if not token_tagged[1].startswith('S'):\n",
        "        tokens.append(f'{token_tagged[0]}/{token_tagged[1]}')\n",
        "\n",
        "freqs = FreqDist(tokens)\n",
        "print(freqs.most_common(15))\n",
        "\n",
        "texts = Text(tokens)\n",
        "texts.concordance('당나귀/NNG')"
      ],
      "metadata": {
        "id": "7GHJuTse1GYy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}